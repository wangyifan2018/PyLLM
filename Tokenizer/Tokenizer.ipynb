{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从Bert开始，NLP任务都改用subword的分词方法，主要包括：BPE、WordPiece、ULM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE训练\n",
    "\n",
    "训练流程\n",
    "\n",
    "1. 将数据集拆成词，并统计词频\n",
    "2. 首先将数据集拆成词，并统计词频 （为了区分词的开始、结束，可以末尾添加</w>，或者在句中添加##等特殊符号）\n",
    "  举例如下：\n",
    "      {\"l o w </w>\":5, \"l o w e r </w>\":2, \"n e w e s t </w>\": 6, \"w i d e s t </w>\": 3}\n",
    "      词典大小：{\"l\",\"o\",\"w\",\"e\",\"r\",\"n\",\"s\",\"t\",\"i\",\"d\",\"</w>\"}\n",
    "3. 从统计出的词频中，统计bigram最多的次数，组成新的单元\n",
    "lo: 5+2=7, ow: 5+2=7, w</w>: 5\n",
    "we: 2+6=8, er: 2, r</w>: 2\n",
    "ne: 6, ew: 6, es: 6+3=9, st: 6+3=9, t</w>: 6+3=9\n",
    "wi: 3, id: 3, de: 3\n",
    "es、st、t</w>都为9，可任选一个，变成{\"l o w </w>\":5, \"l o w e r </w>\":2, \"n e w es t </w>\": 6, \"w i d es t </w>\": 3}\n",
    "词典大小：{\"l\",\"o\",\"w\",\"e\",\"r\",\"n\",\"es\",\"t\",\"i\",\"d\", \"</w>\"}\n",
    "4. 继续第3步，直到：达到预设的迭代次数 或 达到预设的Subword词表大小 或 下一个最高频的字节对出现频率为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 训练代码实现如下：\n",
    "\n",
    "# 首先我们需要一个简单的分词器，将句子拆分成单词（如根据空格、标点进行拆分）\n",
    "import toolz, re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def wordpunct_tokenize(text):\n",
    "    _pattern = r\"\\w+|[^\\w\\s]+\"\n",
    "    _regexp = re.compile(_pattern, flags=re.UNICODE | re.MULTILINE | re.DOTALL)\n",
    "    return _regexp.findall(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 词典训练过程如下（BPETokenizer.fit()）\n",
    "\n",
    "class BPETokenizer():\n",
    "    special = ['<UNK>', '<PAD>', '<END>', '<MASK>']\n",
    "\n",
    "    def __init__(self, vocab_size=1000, lowercase=True, basic_tokenizer=wordpunct_tokenize):\n",
    "        self.lowercase = lowercase\n",
    "        self.vocab_size = vocab_size\n",
    "        self.basic_tokenizer = basic_tokenizer\n",
    "\n",
    "    def fit(self, corpus: list, max_steps=10000, out_fn='vocab.txt'):\n",
    "        '''\n",
    "        分词器训练，返回训练得到的vocabulary\n",
    "        '''\n",
    "\n",
    "        ############### 统计初始词典 ###############\n",
    "        if self.lowercase:\n",
    "            corpus = [s.lower() for s in corpus]\n",
    "        word_corpus = Counter([tuple(data) + (\"</w>\",) for data in toolz.concat(map(self.basic_tokenizer, corpus))])\n",
    "        vocab = self._count_vocab(word_corpus)\n",
    "\n",
    "        ############### 逐步合并初始词典中的高频二元组 ###############\n",
    "        for i in range(max_steps):\n",
    "            word_corpus, bi_cnt = self._fit_step(word_corpus)\n",
    "            vocab = self._count_vocab(word_corpus)\n",
    "            if len(vocab) >= self.vocab_size or bi_cnt < 0: break\n",
    "\n",
    "        ############### 将一些特殊词加入最终的词典 ###############\n",
    "        for s in self.special:\n",
    "            if s not in vocab:\n",
    "                vocab.insert(0, (s, 99999))\n",
    "\n",
    "        ############### 导出词典 ###############\n",
    "        with open(out_fn, 'w') as f:\n",
    "            f.write('\\n'.join([w for w, _ in vocab]))\n",
    "\t\tself.vocab = [token for token, _ in vocab]\n",
    "        return vocab\n",
    "\n",
    "    def _count_vocab(self, word_corpus):\n",
    "        _r = Counter([data for data in toolz.concat([word * cnt for word, cnt in word_corpus.items()])])\n",
    "        _r = sorted(_r.items(), key=lambda x: -x[1])\n",
    "        return _r\n",
    "\n",
    "    def _fit_step(self, word_corpus):\n",
    "        ngram = 2\n",
    "        bigram_counter = Counter()\n",
    "\n",
    "        ############### 以步长1，窗口尺寸2，在每个单词上滚动，统计二元组频次 ###############\n",
    "        for token, count in word_corpus.items():\n",
    "            if len(token) < 2: continue\n",
    "            for bigram in toolz.sliding_window(ngram, token):\n",
    "                bigram_counter[bigram] += count\n",
    "\n",
    "        ############### 选出频次最大的二元组 ###############\n",
    "        if len(bigram_counter) > 0:\n",
    "            max_bigram = max(bigram_counter, key=bigram_counter.get)\n",
    "        else:\n",
    "            return word_corpus, -1\n",
    "        bi_cnt = bigram_counter.get(max_bigram)\n",
    "\n",
    "        ############### 从corpus中将最大二元组出现的地方替换成一个token ###############\n",
    "        for token in word_corpus:\n",
    "            _new_token = tuple(' '.join(token).replace(' '.join(max_bigram), ''.join(max_bigram)).split(' '))\n",
    "            if _new_token != token:\n",
    "                word_corpus[_new_token] = word_corpus[token]\n",
    "                word_corpus.pop(token)\n",
    "        return word_corpus, bi_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 统计初始词典\n",
    "  - 首先如果设置了lowercase，会将大/小写同等对待（统一转换成小写）\n",
    "  - 然后通过对句子简单分词，并数量统计，得到Counter结构，如Counter({('n', 'e', 'w', 'e', 's', 't', '</w>'): 6, ('l', 'o', 'w', '</w>'): 5, ('w', 'i', 'd', 'e', 's', 't', '</w>'): 3, ('l', 'o', 'w', 'e', 'r', '</w>'): 2})\n",
    "  - 在_count_vocab中统计处目前的词典为[('e', 17), ('w', 16), ('</w>', 16), ('s', 9), ('t', 9), ('l', 7), ('o', 7), ('n', 6), ('i', 3), ('d', 3), ('r', 2)]\n",
    "- 持续迭代合并词典中的高频二元组（过程见_fit_step()），并更新vocab。直到 超过迭代步数 或 词典大小满足要求 或 已经没有可合并元素\n",
    "  - 在corpus的每个word中，以步长1，窗口尺寸2，统计出所有二元组token的频次\n",
    "  - 将最大二元组出现的地方合并成一个token\n",
    "- 最后是添加一些特殊词并导出词典\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE分词\n",
    "分词是利用上一步训练好的vocab，将句子切分成词典中的token，分词是一个匹配最长子串的过程\n",
    "\n",
    "- 首先还是利用简单分词器self.basic_tokenzier，将句子分成单词序列\n",
    "- 然后对每个单词，从后往前，依次找到包含在vocab中的最长sub_token\n",
    "  - 对于某个单词，如果任何sub_token都不包含在vocab中，那么当做未登录词\"<UNK>\"\n",
    "\n",
    "\n",
    "分词代码如下：\n",
    "\n",
    "- 重点关注tokenize、encode、decode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(self, text: str, add_post='</w>'):\n",
    "    '''\n",
    "    将text转换成tokens\n",
    "    '''\n",
    "\n",
    "    all_tokens = []\n",
    "    if self.lowercase: text = text.lower()\n",
    "    new_token = []\n",
    "\n",
    "    ############### 简单分词，并遍历token ###############\n",
    "    for token in self.basic_tokenizer(text):\n",
    "        token = list(token)\n",
    "        if add_post:\n",
    "            token = token + [add_post]\n",
    "        start, end = 0, len(token)\n",
    "\n",
    "        ############### 查找最长sub_token ###############\n",
    "        while start < end:\n",
    "            sub_token = ''.join(token[start:end])\n",
    "            if sub_token in self.vocab:\n",
    "                new_token.append(sub_token)\n",
    "                start = end\n",
    "                end = len(token)\n",
    "            elif end - start == 1:\n",
    "                new_token.append('<UNK>')\n",
    "                start = end\n",
    "                end = len(token)\n",
    "            else:\n",
    "                end -= 1\n",
    "    all_tokens.extend(new_token)\n",
    "    return all_tokens\n",
    "\n",
    "def encode(self, text: str):\n",
    "    '''\n",
    "    将text转换成token_ids\n",
    "    '''\n",
    "    tokens_list = self.tokenize(text)\n",
    "    ids_list = [list(map(lambda x: self._token2id[x], tokens)) for tokens in tokens_list]\n",
    "    return ids_list\n",
    "\n",
    "def decode(self, token_ids):\n",
    "    '''\n",
    "    将token_ids还原成text\n",
    "    '''\n",
    "    sentences = []\n",
    "    for ids in token_ids:\n",
    "        sentence = list(map(lambda x: self._id2token[x], ids))\n",
    "        sentence = ''.join(sentence).replace('</w>', ' ')\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def _token2id(self, token):\n",
    "    if token in self.vocab:\n",
    "        return self.vocab.index(token)\n",
    "    return self.vocab.index('<UNK>')\n",
    "\n",
    "def _id2token(self, id):\n",
    "    return self.vocab[id]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
